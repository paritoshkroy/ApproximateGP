# Spectral Approximation Methods {#spectral}

## Hilbert space GP
The Hilbert space method proposed by @solin2020hilbert is a low-rank approximation of a latent Gaussian process based on a finite number of basis function expansions scaled by the square root of spectral density values of the covariance function. The method uses eigenfunctions of the Laplace operator in a compact subset of $\mathbb{R}^{d}$ with Dirichlet boundary conditions on the boundary as the basis functions, and the spectral density functions are evaluated at the square root of the Laplacian eigenvalues. Detailed theory and practical recommendations for a $d$-dimensional GP are available in [@solin2020hilbert] and [@riutort2023practical]. Here we briefly describe the Hilbert space method for approximating a stationary GP $\{z_k(\boldsymbol{s}): \boldsymbol{s} \equiv (s_1,s_2) \in \mathcal{D} \subset \mathbb{R}^{2}\}$, over a two-dimensional spatial domain, with mean zero, marginal variance $\sigma^2$ and isotropic correlation function $C_{\nu}(\cdot, \ell)$. 

The Laplacian needs to be defined in a domain and with Dirichlet boundary conditions on the boundary. [@riutort2023practical] choose a rectangular domain $\mathcal{D}^\star = [-L_1, L_1] \times [-L_2, L_2]$ which contains all the data points, maybe after shifting the coordinates. For example, define the transformation  $S_1 = \max_{i}|s_{1i} - (\min_{i}{s_{1}} + \max_{i}{s_{1}})/2|$, $S_2 = \max_{i}|s_{2i} - (\min_{i}{s_{2}} + \max_{i}{s_{2}})/2$ and $L_1 = c \times S_1, L_2 = c \times S_2$ fulfill the requirements, where $c \ge 1$ is the proportional extension factor or the boundary factor of the approximation. Note that such transformation of coordinates does not change the lengthscale of the correlation function. Let $m_1$ and $m_2$ be the number of basis functions for a two-dimensional domain, such that $m = m_1 \times m_2$ denotes the total number of basis functions to be used in the approximation. Also, $m$ is equal to the number of possible combinations of indices of the basis functions, i.e., the number of 2-tuples from two vectors of indices $(1,2,\ldots,m_1)$ and $(1,2,\ldots,m_2)$. Let $\mathbb{S} \in \mathbb{N}^{m \times 2}$ be the matrix containing all those 2-tuples. Thus for $j = 1, \ldots, m$, we have eigenvalues and eigenfunctions of the Laplace operator as
\begin{align}
\boldsymbol{\lambda}_{j}' &= (\lambda_{j1},\lambda_{j2}) = \left(\frac{\pi\mathbb{S}_{j1}}{2L_1}, \frac{\pi\mathbb{S}_{j2}}{2L_2}\right),\\
\phi_j(\boldsymbol{s}) &= \sqrt{\frac{1}{L_1}}\sin\left(\sqrt{\lambda_{j1}}\, (s_1 + L_1)\right) \; \times \; \sqrt{\frac{1}{L_2}}\sin\left(\sqrt{\lambda_{j2}}\, (s_2 + L_2)\right).
\end{align}
The Hilbert space method approximates a stationary GP $\{z_k(\boldsymbol{s}): \boldsymbol{s} \in \mathcal{D} \subset \mathbb{R}^{2}\}$ through
\begin{align}
\label{eq_hsgp_def}
z_k(\boldsymbol{s}) \approx \sum_{j=1}^{m} \phi_j(\boldsymbol{s})\, \beta_{kj}, 
\end{align}
where $\beta_{jk} \sim \mathcal{N}\left(0, S_{\nu}(\sqrt{\boldsymbol{\lambda}_j}, \ell_k)\right)$ with the spectral density $S_{\nu}(\cdot, \ell_k)$  associated with the covariance function $C_{\nu}(\cdot, \ell_k)$. The resultant covariance function depends on parameters $\ell_k$ through the spectral density $S_{\nu}(\cdot, \ell_k)$. The eigenfunctions are independent of the particular choice of covariance function, including the hyperparameters. Therefore, the distribution of any finite subset of random variables, $\boldsymbol{z}_k = (z_k(\boldsymbol{s}_1),\ldots,z_k(\boldsymbol{s}_n))'$ is approximated by $\boldsymbol{z}_k \sim \mathcal{N}\left(\boldsymbol{0}, \boldsymbol{\Phi} \boldsymbol{\Delta}_k \boldsymbol{\Phi}'\right)$, where $\boldsymbol{\Phi}$ is the $n\times m$ matrix of eigenfunctions with $(i,j)$th elements $\phi_j(\boldsymbol{s}_i)$ and $\boldsymbol{\Delta}_k$ is a $m\times m$ diagonal matrix of the spectral density values $S_{\nu}(\sqrt{\boldsymbol{\lambda}_j}, \ell_k),\, j = 1, \ldots,m$. The computational cost of evaluating the log density of the Hilbert space GP is $\mathcal{O}\left(nm + m\right)$. The value of $m$ is usually much smaller than the number of observations, $n$; thus, parameter space is reduced, making inference faster. 

The accuracy and speed of the approximation depend on the number of basis functions and boundary conditions of the Laplace eigenfunctions. Appropriate values for these two factors will depend on the GP's smoothness or wiggliness, which is described by the lengthscale of the covariance function. [@riutort2023practical] presents some practical recommendations for selecting the number of basis function $m$ and the boundary factor $c$. They empirically discovered a lower bound for $c$ of $c \ge 1.2$ and, within the range of $c$, derived near-linear relationships between $m$, $\ell$ and $c$, for each of the squared-exponential, Mat\'ern 5/2 and Mat\'ern 3/2 covariance functions. They recommend that the choice of $m$ following equations below will result in precise approximations of a unidimensional GP with Mat\'ern 3/2 covariance function: 
\begin{align}
m = 3.42 \, \frac{c}{\ell/S} \Leftrightarrow \ell/S = 3.42 \,\frac{c}{m}
\end{align}
where $C \ge  4.5\, \ell/S$ and $c \ge 1.2$. According to \citet{riutort2023practical}, this formula provides the largest $m$ and $c$, thus could be used as a conservative choice for all Mat\'ern covariance functions with $\nu \ge 3/2$ and likely as a good initial guess for many other covariance functions. The conclusion can be used for each dimension of a GP with a two-dimensional Mat\'ern 3/2 covariance function, but the approximation is more costly than the univariate case. This is because the number of basis functions is then the product of the number of univariate basis functions over each dimension. 
