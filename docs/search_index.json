[["index.html", "A Review of Gaussian Process Approximation Methods in Spatial Models Using Stan Chapter 1 About", " A Review of Gaussian Process Approximation Methods in Spatial Models Using Stan Paritosh Kumar Roy 2026-02-21 Chapter 1 About "],["introduction.html", "Chapter 2 Introduction 2.1 Gaussian processes 2.2 Point-referenced spatial data 2.3 Modeling point-referenced spatial data using GP 2.4 Response GP in Stan 2.5 Hierarchical representation of the above model 2.6 Latent GP in Stan 2.7 Computational complexity in analysing large datasets", " Chapter 2 Introduction 2.1 Gaussian processes A Gaussian process is a random function \\(\\{z(\\mathbf{s}): \\mathbf{s} \\in \\mathcal{D}\\}\\) defined over a \\(d\\)–dimensional surface (domain) \\(\\mathcal{D} \\subset \\mathbb{R}^d\\), any finite number of which have a multivariate normal distribution. Therefore, a Gaussian process can be completely specified by a mean function \\(\\mu(\\mathbf{s}) = \\mathbb{E}\\left[z(\\mathbf{s})\\right]\\) and a covariance function \\(C(\\mathbf{s},\\mathbf{s}&#39;) = \\text{Cov}\\left[z(\\mathbf{s}),z(\\mathbf{s}^\\prime)\\right]\\). The covariance function is commonly specified as the product of a variance parameter and a correlation function, a function of Euclidean distance between locations in \\(\\mathcal{D}\\), that is, \\(C(\\mathbf{s},\\mathbf{s}&#39;) = \\sigma^2 \\rho(\\mathbf{s}, \\mathbf{s}^\\prime)\\), where \\(\\sigma\\) is the marginal standard deviation and \\(\\rho(\\mathbf{s}, \\mathbf{s}^\\prime)\\) is a valid correlation function that depends on the Euclidean distance between \\(\\mathbf{s}\\) and \\(\\mathbf{s}^\\prime\\) in \\(\\mathcal{D}\\). The resulting Gaussian process is stationary and isotropic , and is also called homogeneous Gaussian process . A common example of a valid covariance function is the Mat'ern family of isotropic covariance functions, which is given by \\[ \\begin{align} \\label{Chap2:Maternfunction} \\rho(r, \\nu, \\ell) = \\dfrac{2^{1-\\nu}}{\\Gamma (\\nu)}\\left( \\sqrt{2\\nu}\\, \\dfrac{r}{\\ell} \\right)^{\\nu} \\, K_{\\nu} \\left(\\sqrt{2\\nu} \\, \\dfrac{r}{\\ell}\\right), \\end{align} \\] where \\(r = ||\\mathbf{s}-\\mathbf{s}^\\prime||\\) is the distance between any two locations \\(\\mathbf{s}\\) and \\(\\mathbf{s}^\\prime\\) in \\(\\mathcal{D}\\). The parameter \\(\\nu&gt;0\\) controls the differentiability of the process, and \\(\\ell\\) is called lengthscale, which measures the distance at which the process’s fluctuations begin to repeat. A smaller lengthscale results in a more oscillatory function with rapid changes, capturing fine details in the data, whereas a larger lengthscale produces a smoother function with gradual changes, averaging out smaller fluctuations. In this sense, the lengthscale of a Gaussian process also serves as a measure of the smoothness or roughness of the functions it generates, impacting how it captures patterns and variations in the data. The component \\(K_{\\nu}(\\cdot)\\) in the covariance function is a modified Bessel function of the second kind of order \\(\\nu\\). The process becomes very rough for a small value of \\(\\nu\\) (say, \\(\\nu = 1/2\\)), whereas \\(\\nu = 3/2\\) and \\(\\nu = 5/2\\) are appealing cases for which the processes are once and twice differentiable, respectively, and for \\(\\nu &gt; 7/2\\) the process is very smooth. Also, note that for \\(\\nu \\in \\{1/2, 3/2, 5/2\\}\\) the resultant covariance function has a computationally simple form; however, for \\(\\nu \\notin \\{1/2, 3/2, 5/2\\}\\) it is necessary to compute \\(K_{\\nu}(\\cdot)\\), which is computationally expensive. In practice, the analyst may prefer to fix the parameter \\(\\nu\\) based on subject knowledge. The lengthscale parameter is closely related to the practical range in spatial statistics, indicating the distance over which spatial dependence between observations remains effective. Beyond this range, observations are considered nearly independent, with correlations diminishing to negligible levels. The practical range is crucial for determining appropriate spatial scales for analysis and modeling, ensuring accurate representation of spatial processes. For instance, with \\(\\nu = 3/2\\), the Mat'ern 3/2 correlation function is \\[ \\begin{align} C_{3/2}(r, \\ell) = (1 + \\dfrac{\\sqrt{3}\\, r}{\\ell}) \\exp\\left(-\\dfrac{\\sqrt{3}\\,r}{\\ell}\\right), \\end{align} \\] indicating how spatial correlation decreases with distance, and for distances \\(\\ell/2\\), \\(\\ell\\), \\(2\\ell\\), \\(2.75\\ell\\), and \\(4\\ell\\), the correlations are approximately 0.78, 0.48, 0.14, 0.05, and 0.008, respectively. By definition, for any finite set of locations \\(\\{\\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\} \\in \\mathcal{D}\\) the joint distribution of an \\(n\\)–dimensional vector of possible realizations \\(\\mathbf{z}^\\prime = (z(\\mathbf{s}_1),\\ldots,z(\\mathbf{s}_n))\\) from a Gaussian process follows a multivariate normal distribution, that is, \\[ \\begin{align} f(\\mathbf{z} \\mid \\boldsymbol{\\theta}) \\propto \\dfrac{1}{\\sqrt{|\\sigma^2\\mathbf{B}|}} \\exp\\left\\{-\\dfrac{1}{2\\sigma^2} (\\mathbf{z} - \\boldsymbol{\\mu})^\\prime \\mathbf{B}^{-1}(\\mathbf{z} - \\boldsymbol{\\mu})\\right\\}, \\end{align} \\] where \\(\\boldsymbol{\\mu}^\\prime = (\\mu(\\mathbf{s}_1),\\ldots,\\mu(\\mathbf{s}_n))\\) is a \\(n\\)–dimensional vector of means, and \\(\\mathbf{B}\\) is a \\(n\\)–dimensional correlation matrix with \\((i,j)\\)th element is \\(\\rho(\\mathbf{s}_i,\\mathbf{s}_j)\\). 2.2 Point-referenced spatial data Point-referenced spatial data refers to observations where each data point is associated with a precise location defined by coordinates. The coordinates typically include latitude and longitude for global positioning, easting and northing for local projections, or \\((x,y)\\)–coordinates of a surface. Analyzing point reference data aims to capture variability and correlation in observed phenomena, predict values at unobserved locations, and assess uncertainty. Its applications are widespread in environmental monitoring and geophysical studies. For example, weather stations record temperature, humidity, and air quality at some fixed monitoring sites across a geographical area, and data analysis often aims to obtain a predicted surface of the phenomena by estimating values at unobserved locations. Statistical modeling of point reference data often assumes that the measurement variable is, theoretically, defined at locations that vary continuously across the domain. Thus, it necessitates specifying a random surface . A widely adopted approach involves modeling the surface as a realization of a stochastic process. Gaussian processes provide a practical framework for such modeling, offering a versatile tool for representing the spatial processes that vary continuously across space. The Gaussian processes facilitate straightforward inference and prediction by capturing spatial correlations, interpolating data, modeling variability, and enabling probabilistic inference. In the following, we will explore the application of Gaussian processes in analyzing point-referenced spatial data. 2.3 Modeling point-referenced spatial data using GP Within this framework, observations over a finite set of locations in a spatial domain are assumed to be partial realizations of a spatial Gaussian process \\(\\{y(\\mathbf{s}): \\mathbf{s} \\in \\mathcal{D}\\}\\) that defined on spatial index \\(\\mathbf{s}\\) varying continuously throughout \\(d\\)–dimensional domain \\(\\mathcal{D} \\in \\mathbb{R}^{d}\\). Therefore, the joint distribution of measurements at any finite set of locations is assumed to be multivariate normal, and the properties of the multivariate normal distribution ensure closed-form marginal and conditional distributions, leading to straightforward computation for model fitting and prediction. It assumes that the measurement \\(y(\\mathbf{s})\\), at location \\(\\mathbf{s}\\), be a generated as \\[ \\begin{align} y(\\mathbf{s}) = \\mathbf{x}(\\mathbf{s})^\\prime \\boldsymbol{\\theta} + z(\\mathbf{s}) + \\epsilon(\\mathbf{s}), \\end{align} \\] where \\(\\boldsymbol{\\theta}\\) is a \\(p\\)–dimensional vector of coefficient associated with \\(p\\)–dimensional vector of covariates, \\(\\mathbf{x}(\\mathbf{s})\\), including intercept. The component \\(z(\\mathbf{s})\\) is assumed to be a zero mean isotropic Gaussian process with marginal variance \\(\\sigma^2\\) and correlation function \\(\\rho(\\cdot)\\) capturing the spatial correlation and ensures that \\(y(\\mathbf{s})\\) is defined at every location \\(\\mathbf{s} \\in \\mathcal{D}\\), and \\(\\epsilon(\\mathbf{s})\\) is assumed to be an independent random measurement error which follows a normal distribution with mean zero and variance \\(\\tau^2\\). 2.3.1 Inference Procedure Let \\(\\mathbf{y} = (y(\\mathbf{s}_1),\\ldots, y(\\mathbf{s}_n))^\\prime\\) be the \\(n\\)–dimensional vector of data over \\(n\\) locations \\(\\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) in \\(d\\)–dimensional domain \\(\\mathcal{D} \\in \\mathbb{R}^{d}\\). Using marginalization with respect to \\(\\mathbf{z}^\\prime = (z(\\mathbf{s}_1),\\ldots,z(s_n))\\), it can be shown that \\(\\mathbf{y}\\) is distributed as multivariate normal with \\[ \\begin{align} \\mathbb{E}[\\mathbf{y}] = \\mathbf{X}\\boldsymbol{\\theta} \\qquad \\text{and} \\qquad \\text{Var}[\\mathbf{y}] = \\sigma^2 \\mathbf{B} + \\tau^2\\mathbf{I}, \\end{align} \\] where \\(\\mathbf{X}\\) is a \\(n \\times p\\) design matrix based on the vector of covariates \\(\\mathbf{x}(\\mathbf{s}_i)\\) and \\(\\mathbf{B}\\) is the \\(n\\)–dimensional correlation matrix whose \\((i,j)\\)th elements is \\(\\mathbf{B}_{ij} = \\rho(||\\mathbf{s}_i-\\mathbf{s}_j||)\\). Under the Bayesian paradigm, model specification is complete after assigning a prior distribution for \\(\\boldsymbol{\\beta}\\), \\(\\sigma\\), \\(\\ell\\) and \\(\\tau\\). Then, following Bayes’ theorem, the joint posterior distribution of \\(\\boldsymbol{\\Phi} = \\{\\boldsymbol{\\theta}, \\sigma, \\ell, \\tau\\}\\) is proportional to \\[ \\begin{align} \\pi(\\boldsymbol{\\Phi} \\mid \\mathbf{y}) \\propto \\mathcal{N}\\left(\\mathbf{y} \\mid \\mathbf{X}\\boldsymbol{\\theta}, \\mathbf{V}\\right) \\; \\pi(\\boldsymbol{\\Phi}), \\end{align} \\] where \\(\\mathbf{V} = \\sigma^2 \\mathbf{B} + \\tau^2\\mathbf{I}\\) and \\(\\pi(\\boldsymbol{\\Phi})\\) denotes the prior distribution assigned to \\(\\boldsymbol{\\Phi}\\). In general, the distribution \\(\\pi(\\boldsymbol{\\Phi} \\mid \\mathbf{y})\\) does not have a closed-form, and Markov chain Monte Carlo (MCMC) sampling methods are commonly employed to approximate this distribution. These methods are straightforward to implement using modern statistical computing platforms such as BUGS, JAGS, NIMBLE, and Stan. MCMC methods provide samples from the posterior distribution, which can be used to estimate various summary statistics. Once samples from the posterior distribution are available, predictions to unobserved locations follow straightforwardly. The described inference procedure utilizes a model that is marginalized with respect to the latent GP by integrating out the latent variable \\(\\mathbf{z}\\). This approach allows the model to directly predict the observed responses, which is why it is referred to as a response GP or marginal GP. 2.4 Response GP in Stan data { int&lt;lower=0&gt; n; int&lt;lower=0&gt; p; vector[n] y; matrix[n,p] X; array[n] vector[2] coords; vector&lt;lower=0&gt;[p] scale_theta; real&lt;lower=0&gt; scale_sigma; real&lt;lower=0&gt; scale_tau; real&lt;lower = 0&gt; a; // Shape parameters in the prior for ell real&lt;lower = 0&gt; b; // Scale parameters in the prior for ell } transformed data{ } parameters { vector[p] theta_std; real&lt;lower=0&gt; ell; real&lt;lower=0&gt; sigma_std; real&lt;lower=0&gt; tau_std; } transformed parameters{ vector[p] theta = scale_theta .* theta_std; real sigma = scale_sigma * sigma_std; real tau = scale_sigma * tau_std; } model { theta_std ~ std_normal(); ell ~ inv_gamma(a,b); sigma_std ~ std_normal(); tau_std ~ std_normal(); vector[n] mu = X*theta; matrix[n,n] Sigma = gp_matern32_cov(coords, sigma, ell); matrix[n,n] L = cholesky_decompose(add_diag(Sigma, square(tau))); y ~ multi_normal_cholesky(mu, L); } 2.4.1 Spatial Interpolation One main interest in point-referenced spatial data analysis is obtaining a predicted surface for the process through pointwise prediction. Let \\(\\{\\mathbf{s}_1^\\star, \\ldots, \\mathbf{s}_{n^\\star}^\\star\\}\\) be a set of \\(n^\\star\\) high resoluted grid locations covering \\(\\mathcal{D}\\) and suppose that vector of \\(p\\) covariates values \\(\\mathbf{x}(\\mathbf{s}^\\star)\\) at each site is available. To obtain predicted surface along with reporting uncertainty measures, we need posterior predicted distribution of \\(\\mathbf{y}^\\star = (y(\\mathbf{s}_1^\\star),\\ldots, y(\\mathbf{s}_{n^\\star}^\\star))^\\prime\\), conditional on the observed data \\(\\mathbf{y}\\). For this purpose, consider the joint vector \\((\\mathbf{y}^{\\star\\prime},\\mathbf{y}^\\prime)\\), under a Gaussian process assumption whose distribution is \\((n^\\star + n)\\)–dimensional multivariate normal. Consequently, the conditional distribution \\(\\mathbf{y}^\\star\\) given \\(\\mathbf{y}\\) is \\(n^\\star\\)–dimensional multivariate normal with conditional mean and variance, respectively, given by \\[ \\begin{align} \\mathbb{E}[\\mathbf{y}^\\star \\mid \\mathbf{y}] = \\mathbf{X}^\\star\\boldsymbol{\\theta} + \\mathbf{V}^{\\text{pred-to-obs}} \\mathbf{V}^{-1} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})\\\\ \\text{and} \\text{Var}[\\mathbf{y}^\\star \\mid \\mathbf{y}] = \\mathbf{V}^\\star - \\mathbf{V}^{\\text{pred-to-obs}} \\mathbf{V}^{-1} \\mathbf{V}^{\\text{obs-to-pred}}, \\end{align} \\] which is used to perform the prediction, where \\(\\mathbf{X}^\\star\\) is the \\(n^\\star \\times p\\) design matrix of covariates at prediction locations. The covariance matrix \\(\\mathbf{V}^\\star\\) is equal to \\(\\sigma^2 \\mathbf{B}^\\star + \\tau^2 \\mathbf{I}\\), where \\(\\mathbf{B}^\\star\\) denotes the \\(n^\\star\\)–dimensional spatial correlation matrix among the prediction locations. The component \\(\\mathbf{V}^{\\text{pred-to-obs}}\\) is equal to \\(\\sigma^2 \\mathbf{B}^{\\text{pred-to-obs}}\\), where \\(\\mathbf{B}^\\text{pred-to-obs}\\) denotes the \\(n^\\star \\times n\\) spatial correlation matrix between prediction and observed locations. However, the above joint prediction is computationally expensive as the conditional distribution of a multivariate normal distribution of dimension \\((n^\\star+n)\\), computing conditional mean \\(\\mu_{y(\\mathbf{s}^\\star) \\mid \\mathbf{y}}\\) and variance \\(\\sigma^2_{y(\\mathbf{s}^\\star) \\mid \\mathbf{y}}\\) involves expensive matrix calculations. In practice, this can be avoided by performing predictions for each unobserved location separately. In that case, at a generic prediction location \\(\\mathbf{s}^\\star \\in \\mathcal{D}\\), the posterior predictive distribution of \\(y(\\mathbf{s})\\) at \\(\\mathbf{s}^\\star\\) is given by \\[ \\begin{align} \\pi(y(\\mathbf{s}^\\star) \\mid \\mathbf{y}) = \\int_{\\boldsymbol{\\Phi}} \\mathcal{N}(y(\\mathbf{s}^\\star) \\mid \\mathbf{X}\\boldsymbol{\\theta}, \\mathbf{V}) \\pi(\\boldsymbol{\\Phi} \\mid \\mathbf{y}) \\mathrm{d}\\boldsymbol{\\Phi} \\end{align} \\] This procedure is known as a univariate prediction, each step of which involves calculating the matrix inversion of order \\(n\\) and is still expensive if \\(n\\) is large. 2.4.2 Recovery of the Latent Component One might be interested in the posterior distribution of the latent spatial component \\(z(\\mathbf{s})\\). The inference using joint posterior distribution in equation ignores the estimation of the latent vector \\(\\mathbf{z}^\\prime = (z(\\mathbf{s}_1), \\ldots, z(\\mathbf{s}_n))\\) during model fitting. Nevertheless, we can recover the distribution of vector \\(\\mathbf{z}\\) components via composition sampling once samples from the posterior distribution of the parameters are available. Note that the joint posterior distribution of \\(\\mathbf{z}\\) is \\[ \\begin{align} \\pi(\\mathbf{z} \\mid \\mathbf{y}) &amp;= \\int \\pi(\\boldsymbol{\\Phi}, \\mathbf{z} \\mid \\mathbf{y}) \\; \\mathrm{d} \\boldsymbol{\\Phi}\\\\ &amp;= \\int \\pi(\\mathbf{z} \\mid \\boldsymbol{\\Phi}, \\mathbf{y}) \\; \\pi(\\boldsymbol{\\Phi} \\mid \\mathbf{y}) \\; \\mathrm{d} \\boldsymbol{\\Phi}, \\end{align} \\] and \\[ \\begin{align} \\pi(\\mathbf{z} \\mid \\boldsymbol{\\Phi}, \\mathbf{y}) &amp;\\propto \\mathcal{N}(\\mathbf{z} \\mid \\mathbf{0}, \\sigma^2 \\mathbf{B}) \\; \\mathcal{N}\\left(\\mathbf{y} \\mid \\mathbf{X}\\boldsymbol{\\theta} + \\mathbf{z}, \\tau^2\\mathbf{I}\\right)\\\\ &amp;\\propto \\exp\\left\\{-\\frac{1}{2\\sigma^2} \\mathbf{z}^\\prime \\mathbf{B}^{-1} \\mathbf{z}\\right\\} \\; \\exp\\left\\{-\\frac{1}{2\\tau^2} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta} -\\mathbf{z})^\\prime (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{z})\\right\\} \\nonumber\\\\ &amp;\\propto \\exp\\left\\{-\\frac{1}{2}\\mathbf{z}^\\prime \\left(\\frac{1}{\\tau^2} \\mathbf{I} + \\frac{1}{\\sigma^2}\\mathbf{B}^{-1} \\right) \\mathbf{z} - \\mathbf{z}^\\prime \\left(\\frac{1}{\\tau^2} \\mathbf{I}\\right) (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})\\right\\}, \\end{align} \\] which is the kernel of the multivariate normal distribution with mean and covariance, \\[ \\begin{align} \\mathbb{E}[\\mathbf{z} \\mid \\mathbf{y}] &amp;= \\left(\\dfrac{1}{\\tau^2} \\mathbf{I} + \\dfrac{1}{\\sigma^2}\\mathbf{B}^{-1} \\right)^{-1} \\left(\\dfrac{1}{\\tau^2} \\mathbf{I}\\right) (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}),\\\\ \\text{and}\\; \\text{Var}[\\mathbf{z} \\mid \\mathbf{y}] &amp;= \\left(\\dfrac{1}{\\tau^2} \\mathbf{I} + \\dfrac{1}{\\sigma^2}\\mathbf{B}^{-1} \\right)^{-1}, \\end{align} \\] respectively. Therefore, posterior samples for \\(\\mathbf{z}\\) can be obtained by drawing samples from \\(\\pi(\\mathbf{z} \\mid \\boldsymbol{\\Phi}, \\mathbf{y})\\) one-for-one for each posterior sample of \\(\\boldsymbol{\\Phi}\\). These are post-MCMC calculations; hence, sampling is not very expensive. Given the posterior samples for \\(\\mathbf{z}\\) associated with observed locations and \\(\\boldsymbol{\\Phi}\\), it is also possible to obtain samples of the distribution of \\(n^\\star\\)–dimensional vector \\(\\mathbf{z}^\\star\\) of the values of \\(z(\\mathbf{s})\\) at unobserved locations \\(\\mathbf{s}_{1}^\\star, \\ldots, \\mathbf{s}_{n^\\star}^\\star\\) via composition sampling. The procedure involves assuming joint vectors \\((\\mathbf{z}^{\\star\\prime}, \\mathbf{z}^\\prime)\\) which follows \\((n^\\star + n)\\)–dimensional multivariate normal distribution and conditional distribution of \\(\\mathbf{z}^\\star\\) given \\(\\mathbf{z}\\) is used to draw samples for \\(\\mathbf{z}^\\star\\). The conditional distribution is \\(n^\\star\\)–dimensional multivariate normal with mean \\[ \\begin{align} \\mathbf{E}[\\mathbf{z}^\\star \\mid \\mathbf{z}] = \\mathbf{B}^{\\text{pred-to-obs}} \\mathbf{B}^{-1} \\mathbf{z} \\end{align} \\] and variance \\[ \\begin{align} \\text{Var}[\\mathbf{z}^\\star \\mid \\mathbf{z}] = \\sigma^2 (\\mathbf{B}^\\star - \\mathbf{B}^{\\text{pred-to-obs}} \\mathbf{B}^{-1} \\mathbf{B}^{\\text{obs-to-pred}}). \\end{align} \\] 2.5 Hierarchical representation of the above model Note that the model above specification is referred to as the marginal or response Gaussian model, and the inference and prediction procedures are outlined based on it. However, this model can be represented hierarchically as follows: \\[ \\begin{align} \\mathbf{y} \\mid \\boldsymbol{\\theta}, \\mathbf{z} \\sim \\mathcal{N} \\left(\\mathbf{X}\\boldsymbol{\\theta} + \\mathbf{z}, \\tau^2\\mathbf{I}\\right), \\end{align} \\] \\[\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{B}).\\] In practice, the response Gaussian process model is often preferred for efficient parameter estimation, as it circumvents the need to estimate the latent vector \\(\\mathbf{z}\\) directly. Instead, in a Bayesian analysis, once posterior samples for the parameters are obtained, estimates for \\(\\mathbf{z}\\) can be recovered through composition sampling techniques. 2.6 Latent GP in Stan data { int&lt;lower=0&gt; n; int&lt;lower=0&gt; p; vector[n] y; matrix[n,p] X; array[n] vector[2] coords; vector&lt;lower=0&gt;[p] scale_theta; real&lt;lower=0&gt; scale_sigma; real&lt;lower=0&gt; scale_tau; real&lt;lower=0&gt; a; real&lt;lower=0&gt; b; } transformed data{ } parameters { vector[p] theta_std; real&lt;lower=0&gt; ell; real&lt;lower=0&gt; sigma_std; real&lt;lower=0&gt; tau_std; vector[n] noise; } transformed parameters{ vector[p] theta = scale_theta .* theta_std; real sigma = scale_sigma * sigma_std; real tau = scale_sigma * tau_std; //vector[n] z = cholesky_decompose(add_diag(gp_matern32_cov(coords, sigma, ell), 1e-8)) * noise; vector[n] z = cholesky_decompose(add_diag(gp_exponential_cov(coords, sigma, ell), 1e-8)) * noise; //matrix[n,n] Sigma = gp_exponential_cov(coords, sigma, phi); //matrix[n,n] V = add_diag(V, 1e-8); //matrix[n,n] L = cholesky_decompose(V); //vector[n] z = L * noise; } model { theta_std ~ std_normal(); ell ~ inv_gamma(a,b); sigma_std ~ std_normal(); tau_std ~ std_normal(); noise ~ std_normal(); vector[n] mu = X*theta; y ~ normal(mu + z, tau); } generated quantities{ } 2.7 Computational complexity in analysing large datasets The evaluation of the likelihood of a GP-based model requires the inversion of an \\(n \\times n\\) covariance matrix, which has a computational complexity of \\(\\mathcal{O}(n^3)\\), making it impractical for analyzing large datasets. "],["lowrank.html", "Chapter 3 Low Rank (Basis Function) Approximation 3.1 Basis Function Approximation 3.2 Predictive Gaussian process 3.3 Fixed Rank Kriging 3.4 Chapters and sub-chapters 3.5 Captioned figures and tables", " Chapter 3 Low Rank (Basis Function) Approximation 3.1 Basis Function Approximation 3.2 Predictive Gaussian process 3.3 Fixed Rank Kriging Cross-references make it easier for your readers to find and link to elements in your book. 3.4 Chapters and sub-chapters There are two steps to cross-reference any heading: Label the heading: # Hello world {#nice-label}. Leave the label off if you like the automated heading generated based on your heading title: for example, # Hello world = # Hello world {#hello-world}. To label an un-numbered heading, use: # Hello world {-#nice-label} or {# Hello world .unnumbered}. Next, reference the labeled heading anywhere in the text using \\@ref(nice-label); for example, please see Chapter 3. If you prefer text as the link instead of a numbered reference use: any text you want can go here. 3.5 Captioned figures and tables Figures and tables with captions can also be cross-referenced from elsewhere in your book using \\@ref(fig:chunk-label) and \\@ref(tab:chunk-label), respectively. See Figure 3.1. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 3.1: Here is a nice figure! Don’t miss Table 3.1. knitr::kable( head(pressure, 10), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 3.1: Here is a nice table! temperature pressure 0 0.0002 20 0.0012 40 0.0060 60 0.0300 80 0.0900 100 0.2700 120 0.7500 140 1.8500 160 4.2000 180 8.8000 "],["sparse.html", "Chapter 4 Sparse Covariance or Precision Methods 4.1 SPDE Approach 4.2 Vecchia approximation 4.3 Nearest neighbor Gaussian process 4.4 Sparse Vecchia Gaussian process 4.5 Hierarchical Vecchia Gaussian process", " Chapter 4 Sparse Covariance or Precision Methods 4.1 SPDE Approach 4.2 Vecchia approximation 4.3 Nearest neighbor Gaussian process 4.4 Sparse Vecchia Gaussian process 4.5 Hierarchical Vecchia Gaussian process You can add parts to organize one or more book chapters together. Parts can be inserted at the top of an .Rmd file, before the first-level chapter heading in that same file. Add a numbered part: # (PART) Act one {-} (followed by # A chapter) Add an unnumbered part: # (PART\\*) Act one {-} (followed by # A chapter) Add an appendix as a special kind of un-numbered part: # (APPENDIX) Other stuff {-} (followed by # A chapter). Chapters in an appendix are prepended with letters instead of numbers. "],["spectral.html", "Chapter 5 Spectral Approximation Methods 5.1 Hilbert space GP", " Chapter 5 Spectral Approximation Methods 5.1 Hilbert space GP The Hilbert space method proposed by Solin and Särkkä (2020) is a low-rank approximation of a latent Gaussian process based on a finite number of basis function expansions scaled by the square root of spectral density values of the covariance function. The method uses eigenfunctions of the Laplace operator in a compact subset of \\(\\mathbb{R}^{d}\\) with Dirichlet boundary conditions on the boundary as the basis functions, and the spectral density functions are evaluated at the square root of the Laplacian eigenvalues. Detailed theory and practical recommendations for a \\(d\\)-dimensional GP are available in (Solin and Särkkä 2020) and (Riutort-Mayol et al. 2023). Here we briefly describe the Hilbert space method for approximating a stationary GP \\(\\{z_k(\\boldsymbol{s}): \\boldsymbol{s} \\equiv (s_1,s_2) \\in \\mathcal{D} \\subset \\mathbb{R}^{2}\\}\\), over a two-dimensional spatial domain, with mean zero, marginal variance \\(\\sigma^2\\) and isotropic correlation function \\(C_{\\nu}(\\cdot, \\ell)\\). The Laplacian needs to be defined in a domain and with Dirichlet boundary conditions on the boundary. (Riutort-Mayol et al. 2023) choose a rectangular domain \\(\\mathcal{D}^\\star = [-L_1, L_1] \\times [-L_2, L_2]\\) which contains all the data points, maybe after shifting the coordinates. For example, define the transformation \\(S_1 = \\max_{i}|s_{1i} - (\\min_{i}{s_{1}} + \\max_{i}{s_{1}})/2|\\), \\(S_2 = \\max_{i}|s_{2i} - (\\min_{i}{s_{2}} + \\max_{i}{s_{2}})/2\\) and \\(L_1 = c \\times S_1, L_2 = c \\times S_2\\) fulfill the requirements, where \\(c \\ge 1\\) is the proportional extension factor or the boundary factor of the approximation. Note that such transformation of coordinates does not change the lengthscale of the correlation function. Let \\(m_1\\) and \\(m_2\\) be the number of basis functions for a two-dimensional domain, such that \\(m = m_1 \\times m_2\\) denotes the total number of basis functions to be used in the approximation. Also, \\(m\\) is equal to the number of possible combinations of indices of the basis functions, i.e., the number of 2-tuples from two vectors of indices \\((1,2,\\ldots,m_1)\\) and \\((1,2,\\ldots,m_2)\\). Let \\(\\mathbb{S} \\in \\mathbb{N}^{m \\times 2}\\) be the matrix containing all those 2-tuples. Thus for \\(j = 1, \\ldots, m\\), we have eigenvalues and eigenfunctions of the Laplace operator as \\[\\begin{align} \\boldsymbol{\\lambda}_{j}&#39; &amp;= (\\lambda_{j1},\\lambda_{j2}) = \\left(\\frac{\\pi\\mathbb{S}_{j1}}{2L_1}, \\frac{\\pi\\mathbb{S}_{j2}}{2L_2}\\right),\\\\ \\phi_j(\\boldsymbol{s}) &amp;= \\sqrt{\\frac{1}{L_1}}\\sin\\left(\\sqrt{\\lambda_{j1}}\\, (s_1 + L_1)\\right) \\; \\times \\; \\sqrt{\\frac{1}{L_2}}\\sin\\left(\\sqrt{\\lambda_{j2}}\\, (s_2 + L_2)\\right). \\end{align}\\] The Hilbert space method approximates a stationary GP \\(\\{z_k(\\boldsymbol{s}): \\boldsymbol{s} \\in \\mathcal{D} \\subset \\mathbb{R}^{2}\\}\\) through \\[\\begin{align} \\label{eq_hsgp_def} z_k(\\boldsymbol{s}) \\approx \\sum_{j=1}^{m} \\phi_j(\\boldsymbol{s})\\, \\beta_{kj}, \\end{align}\\] where \\(\\beta_{jk} \\sim \\mathcal{N}\\left(0, S_{\\nu}(\\sqrt{\\boldsymbol{\\lambda}_j}, \\ell_k)\\right)\\) with the spectral density \\(S_{\\nu}(\\cdot, \\ell_k)\\) associated with the covariance function \\(C_{\\nu}(\\cdot, \\ell_k)\\). The resultant covariance function depends on parameters \\(\\ell_k\\) through the spectral density \\(S_{\\nu}(\\cdot, \\ell_k)\\). The eigenfunctions are independent of the particular choice of covariance function, including the hyperparameters. Therefore, the distribution of any finite subset of random variables, \\(\\boldsymbol{z}_k = (z_k(\\boldsymbol{s}_1),\\ldots,z_k(\\boldsymbol{s}_n))&#39;\\) is approximated by \\(\\boldsymbol{z}_k \\sim \\mathcal{N}\\left(\\boldsymbol{0}, \\boldsymbol{\\Phi} \\boldsymbol{\\Delta}_k \\boldsymbol{\\Phi}&#39;\\right)\\), where \\(\\boldsymbol{\\Phi}\\) is the \\(n\\times m\\) matrix of eigenfunctions with \\((i,j)\\)th elements \\(\\phi_j(\\boldsymbol{s}_i)\\) and \\(\\boldsymbol{\\Delta}_k\\) is a \\(m\\times m\\) diagonal matrix of the spectral density values \\(S_{\\nu}(\\sqrt{\\boldsymbol{\\lambda}_j}, \\ell_k),\\, j = 1, \\ldots,m\\). The computational cost of evaluating the log density of the Hilbert space GP is \\(\\mathcal{O}\\left(nm + m\\right)\\). The value of \\(m\\) is usually much smaller than the number of observations, \\(n\\); thus, parameter space is reduced, making inference faster. The accuracy and speed of the approximation depend on the number of basis functions and boundary conditions of the Laplace eigenfunctions. Appropriate values for these two factors will depend on the GP’s smoothness or wiggliness, which is described by the lengthscale of the covariance function. (Riutort-Mayol et al. 2023) presents some practical recommendations for selecting the number of basis function \\(m\\) and the boundary factor \\(c\\). They empirically discovered a lower bound for \\(c\\) of \\(c \\ge 1.2\\) and, within the range of \\(c\\), derived near-linear relationships between \\(m\\), \\(\\ell\\) and \\(c\\), for each of the squared-exponential, Mat'ern 5/2 and Mat'ern 3/2 covariance functions. They recommend that the choice of \\(m\\) following equations below will result in precise approximations of a unidimensional GP with Mat'ern 3/2 covariance function: \\[\\begin{align} m = 3.42 \\, \\frac{c}{\\ell/S} \\Leftrightarrow \\ell/S = 3.42 \\,\\frac{c}{m} \\end{align}\\] where \\(C \\ge 4.5\\, \\ell/S\\) and \\(c \\ge 1.2\\). According to , this formula provides the largest \\(m\\) and \\(c\\), thus could be used as a conservative choice for all Mat'ern covariance functions with \\(\\nu \\ge 3/2\\) and likely as a good initial guess for many other covariance functions. The conclusion can be used for each dimension of a GP with a two-dimensional Mat'ern 3/2 covariance function, but the approximation is more costly than the univariate case. This is because the number of basis functions is then the product of the number of univariate basis functions over each dimension. References Riutort-Mayol, Gabriel, Paul-Christian Bürkner, Michael R Andersen, Arno Solin, and Aki Vehtari. 2023. “Practical Hilbert space approximate Bayesian Gaussian processes for probabilistic programming.” Statistics and Computing 33 (1): 17. Solin, Arno, and Simo Särkkä. 2020. “Hilbert space methods for reduced-rank Gaussian process regression.” Statistics and Computing 30 (2): 419–46. "],["variational-bayesian-inference.html", "Chapter 6 Variational Bayesian Inference 6.1 Equations 6.2 Theorems and proofs 6.3 Callout blocks", " Chapter 6 Variational Bayesian Inference 6.1 Equations Here is an equation. \\[\\begin{equation} f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k} \\tag{6.1} \\end{equation}\\] You may refer to using \\@ref(eq:binom), like see Equation (6.1). 6.2 Theorems and proofs Labeled theorems can be referenced in text using \\@ref(thm:tri), for example, check out this smart theorem 6.1. Theorem 6.1 For a right triangle, if \\(c\\) denotes the length of the hypotenuse and \\(a\\) and \\(b\\) denote the lengths of the other two sides, we have \\[a^2 + b^2 = c^2\\] Read more here https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html. 6.3 Callout blocks The R Markdown Cookbook provides more help on how to use custom blocks to design your own callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html Riutort-Mayol, Gabriel, Paul-Christian Bürkner, Michael R Andersen, Arno Solin, and Aki Vehtari. 2023. “Practical Hilbert space approximate Bayesian Gaussian processes for probabilistic programming.” Statistics and Computing 33 (1): 17. Solin, Arno, and Simo Särkkä. 2020. “Hilbert space methods for reduced-rank Gaussian process regression.” Statistics and Computing 30 (2): 419–46. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
