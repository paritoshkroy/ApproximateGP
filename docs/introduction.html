<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Introduction | A Review of Gaussian Process Approximation Methods in Spatial Models Using Stan</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Introduction | A Review of Gaussian Process Approximation Methods in Spatial Models Using Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="github-repo" content="paritoshkroy/ApproximateGP" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Introduction | A Review of Gaussian Process Approximation Methods in Spatial Models Using Stan" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  

<meta name="author" content="Paritosh Kumar Roy" />


<meta name="date" content="2026-02-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="lowrank.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Review of Gaussian Process Approximation Methods in Spatial Models Using Stan</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#gaussian-processes"><i class="fa fa-check"></i><b>2.1</b> Gaussian processes</a></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#point-referenced-spatial-data"><i class="fa fa-check"></i><b>2.2</b> Point-referenced spatial data</a></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#modeling-point-referenced-spatial-data-using-gp"><i class="fa fa-check"></i><b>2.3</b> Modeling point-referenced spatial data using GP</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introduction.html"><a href="introduction.html#inference-procedure"><i class="fa fa-check"></i><b>2.3.1</b> Inference Procedure</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#response-gp-in-stan"><i class="fa fa-check"></i><b>2.4</b> Response GP in Stan</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introduction.html"><a href="introduction.html#spatial-interpolation"><i class="fa fa-check"></i><b>2.4.1</b> Spatial Interpolation</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction.html"><a href="introduction.html#recovery-of-the-latent-component"><i class="fa fa-check"></i><b>2.4.2</b> Recovery of the Latent Component</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction.html"><a href="introduction.html#hierarchical-representation-of-the-above-model"><i class="fa fa-check"></i><b>2.5</b> Hierarchical representation of the above model</a></li>
<li class="chapter" data-level="2.6" data-path="introduction.html"><a href="introduction.html#latent-gp-in-stan"><i class="fa fa-check"></i><b>2.6</b> Latent GP in Stan</a></li>
<li class="chapter" data-level="2.7" data-path="introduction.html"><a href="introduction.html#computational-complexity-in-analysing-large-datasets"><i class="fa fa-check"></i><b>2.7</b> Computational complexity in analysing large datasets</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="lowrank.html"><a href="lowrank.html"><i class="fa fa-check"></i><b>3</b> Low Rank (Basis Function) Approximation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="lowrank.html"><a href="lowrank.html#basis-function-approximation"><i class="fa fa-check"></i><b>3.1</b> Basis Function Approximation</a></li>
<li class="chapter" data-level="3.2" data-path="lowrank.html"><a href="lowrank.html#predictive-gaussian-process"><i class="fa fa-check"></i><b>3.2</b> Predictive Gaussian process</a></li>
<li class="chapter" data-level="3.3" data-path="lowrank.html"><a href="lowrank.html#fixed-rank-kriging"><i class="fa fa-check"></i><b>3.3</b> Fixed Rank Kriging</a></li>
<li class="chapter" data-level="3.4" data-path="lowrank.html"><a href="lowrank.html#chapters-and-sub-chapters"><i class="fa fa-check"></i><b>3.4</b> Chapters and sub-chapters</a></li>
<li class="chapter" data-level="3.5" data-path="lowrank.html"><a href="lowrank.html#captioned-figures-and-tables"><i class="fa fa-check"></i><b>3.5</b> Captioned figures and tables</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sparse.html"><a href="sparse.html"><i class="fa fa-check"></i><b>4</b> Sparse Covariance or Precision Methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sparse.html"><a href="sparse.html#spde-approach"><i class="fa fa-check"></i><b>4.1</b> SPDE Approach</a></li>
<li class="chapter" data-level="4.2" data-path="sparse.html"><a href="sparse.html#vecchia-approximation"><i class="fa fa-check"></i><b>4.2</b> Vecchia approximation</a></li>
<li class="chapter" data-level="4.3" data-path="sparse.html"><a href="sparse.html#nearest-neighbor-gaussian-process"><i class="fa fa-check"></i><b>4.3</b> Nearest neighbor Gaussian process</a></li>
<li class="chapter" data-level="4.4" data-path="sparse.html"><a href="sparse.html#sparse-vecchia-gaussian-process"><i class="fa fa-check"></i><b>4.4</b> Sparse Vecchia Gaussian process</a></li>
<li class="chapter" data-level="4.5" data-path="sparse.html"><a href="sparse.html#hierarchical-vecchia-gaussian-process"><i class="fa fa-check"></i><b>4.5</b> Hierarchical Vecchia Gaussian process</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="spectral.html"><a href="spectral.html"><i class="fa fa-check"></i><b>5</b> Spectral Approximation Methods</a>
<ul>
<li class="chapter" data-level="5.1" data-path="spectral.html"><a href="spectral.html#hilbert-space-gp"><i class="fa fa-check"></i><b>5.1</b> Hilbert space GP</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="variational-bayesian-inference.html"><a href="variational-bayesian-inference.html"><i class="fa fa-check"></i><b>6</b> Variational Bayesian Inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="variational-bayesian-inference.html"><a href="variational-bayesian-inference.html#equations"><i class="fa fa-check"></i><b>6.1</b> Equations</a></li>
<li class="chapter" data-level="6.2" data-path="variational-bayesian-inference.html"><a href="variational-bayesian-inference.html#theorems-and-proofs"><i class="fa fa-check"></i><b>6.2</b> Theorems and proofs</a></li>
<li class="chapter" data-level="6.3" data-path="variational-bayesian-inference.html"><a href="variational-bayesian-inference.html#callout-blocks"><i class="fa fa-check"></i><b>6.3</b> Callout blocks</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://paritoshkroy.github.io/ApproximateGP/" target="blank">Published here</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Review of Gaussian Process Approximation Methods in Spatial Models Using Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Introduction<a href="introduction.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="gaussian-processes" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Gaussian processes<a href="introduction.html#gaussian-processes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A Gaussian process is a random function <span class="math inline">\(\{z(\mathbf{s}): \mathbf{s} \in \mathcal{D}\}\)</span> defined over a <span class="math inline">\(d\)</span>–dimensional surface (domain) <span class="math inline">\(\mathcal{D} \subset \mathbb{R}^d\)</span>, any finite number of which have a multivariate normal distribution. Therefore, a Gaussian process can be completely specified by a mean function <span class="math inline">\(\mu(\mathbf{s}) = \mathbb{E}\left[z(\mathbf{s})\right]\)</span> and a covariance function <span class="math inline">\(C(\mathbf{s},\mathbf{s}&#39;) = \text{Cov}\left[z(\mathbf{s}),z(\mathbf{s}^\prime)\right]\)</span>. The covariance function is commonly specified as the product of a variance parameter and a correlation function, a function of Euclidean distance between locations in <span class="math inline">\(\mathcal{D}\)</span>, that is, <span class="math inline">\(C(\mathbf{s},\mathbf{s}&#39;) = \sigma^2 \rho(\mathbf{s}, \mathbf{s}^\prime)\)</span>, where <span class="math inline">\(\sigma\)</span> is the marginal standard deviation and <span class="math inline">\(\rho(\mathbf{s}, \mathbf{s}^\prime)\)</span> is a valid correlation function that depends on the Euclidean distance between <span class="math inline">\(\mathbf{s}\)</span> and <span class="math inline">\(\mathbf{s}^\prime\)</span> in <span class="math inline">\(\mathcal{D}\)</span>. The resulting Gaussian process is stationary and isotropic , and is also called homogeneous Gaussian process .</p>
<p>A common example of a valid covariance function is the Mat'ern family of isotropic covariance functions, which is given by
<span class="math display">\[
\begin{align}
\label{Chap2:Maternfunction}
\rho(r, \nu, \ell) = \dfrac{2^{1-\nu}}{\Gamma (\nu)}\left( \sqrt{2\nu}\, \dfrac{r}{\ell} \right)^{\nu} \, K_{\nu} \left(\sqrt{2\nu} \, \dfrac{r}{\ell}\right),
\end{align}
\]</span>
where <span class="math inline">\(r = ||\mathbf{s}-\mathbf{s}^\prime||\)</span> is the distance between any two locations <span class="math inline">\(\mathbf{s}\)</span> and <span class="math inline">\(\mathbf{s}^\prime\)</span> in <span class="math inline">\(\mathcal{D}\)</span>. The parameter <span class="math inline">\(\nu&gt;0\)</span> controls the differentiability of the process, and <span class="math inline">\(\ell\)</span> is called lengthscale, which measures the distance at which the process’s fluctuations begin to repeat. A smaller lengthscale results in a more oscillatory function with rapid changes, capturing fine details in the data, whereas a larger lengthscale produces a smoother function with gradual changes, averaging out smaller fluctuations. In this sense, the lengthscale of a Gaussian process also serves as a measure of the smoothness or roughness of the functions it generates, impacting how it captures patterns and variations in the data. The component <span class="math inline">\(K_{\nu}(\cdot)\)</span> in the covariance function is a modified Bessel function of the second kind of order <span class="math inline">\(\nu\)</span>. The process becomes very rough for a small value of <span class="math inline">\(\nu\)</span> (say, <span class="math inline">\(\nu = 1/2\)</span>), whereas <span class="math inline">\(\nu = 3/2\)</span> and <span class="math inline">\(\nu = 5/2\)</span> are appealing cases for which the processes are once and twice differentiable, respectively, and for <span class="math inline">\(\nu &gt; 7/2\)</span> the process is very smooth. Also, note that for <span class="math inline">\(\nu \in \{1/2, 3/2, 5/2\}\)</span> the resultant covariance function has a computationally simple form; however, for <span class="math inline">\(\nu \notin \{1/2, 3/2, 5/2\}\)</span> it is necessary to compute <span class="math inline">\(K_{\nu}(\cdot)\)</span>, which is computationally expensive. In practice, the analyst may prefer to fix the parameter <span class="math inline">\(\nu\)</span> based on subject knowledge.</p>
<p>The lengthscale parameter is closely related to the practical range in spatial statistics, indicating the distance over which spatial dependence between observations remains effective. Beyond this range, observations are considered nearly independent, with correlations diminishing to negligible levels. The practical range is crucial for determining appropriate spatial scales for analysis and modeling, ensuring accurate representation of spatial processes. For instance, with <span class="math inline">\(\nu = 3/2\)</span>, the Mat'ern 3/2 correlation function is
<span class="math display">\[
\begin{align}
C_{3/2}(r, \ell) = (1 + \dfrac{\sqrt{3}\, r}{\ell}) \exp\left(-\dfrac{\sqrt{3}\,r}{\ell}\right),
\end{align}
\]</span>
indicating how spatial correlation decreases with distance, and for distances <span class="math inline">\(\ell/2\)</span>, <span class="math inline">\(\ell\)</span>, <span class="math inline">\(2\ell\)</span>, <span class="math inline">\(2.75\ell\)</span>, and <span class="math inline">\(4\ell\)</span>, the correlations are approximately 0.78, 0.48, 0.14, 0.05, and 0.008, respectively.</p>
<p>By definition, for any finite set of locations <span class="math inline">\(\{\mathbf{s}_1,\ldots,\mathbf{s}_n\} \in \mathcal{D}\)</span> the joint distribution of an <span class="math inline">\(n\)</span>–dimensional vector of possible realizations <span class="math inline">\(\mathbf{z}^\prime = (z(\mathbf{s}_1),\ldots,z(\mathbf{s}_n))\)</span> from a Gaussian process follows a multivariate normal distribution, that is,
<span class="math display">\[
\begin{align}
f(\mathbf{z} \mid \boldsymbol{\theta}) \propto \dfrac{1}{\sqrt{|\sigma^2\mathbf{B}|}} \exp\left\{-\dfrac{1}{2\sigma^2} (\mathbf{z} - \boldsymbol{\mu})^\prime \mathbf{B}^{-1}(\mathbf{z} - \boldsymbol{\mu})\right\},
\end{align}
\]</span>
where <span class="math inline">\(\boldsymbol{\mu}^\prime = (\mu(\mathbf{s}_1),\ldots,\mu(\mathbf{s}_n))\)</span> is a <span class="math inline">\(n\)</span>–dimensional vector of means, and <span class="math inline">\(\mathbf{B}\)</span> is a <span class="math inline">\(n\)</span>–dimensional correlation matrix with <span class="math inline">\((i,j)\)</span>th element is <span class="math inline">\(\rho(\mathbf{s}_i,\mathbf{s}_j)\)</span>.</p>
</div>
<div id="point-referenced-spatial-data" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Point-referenced spatial data<a href="introduction.html#point-referenced-spatial-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Point-referenced spatial data refers to observations where each data point is associated with a precise location defined by coordinates. The coordinates typically include latitude and longitude for global positioning, easting and northing for local projections, or <span class="math inline">\((x,y)\)</span>–coordinates of a surface. Analyzing point reference data aims to capture variability and correlation in observed phenomena, predict values at unobserved locations, and assess uncertainty. Its applications are widespread in environmental monitoring and geophysical studies. For example, weather stations record temperature, humidity, and air quality at some fixed monitoring sites across a geographical area, and data analysis often aims to obtain a predicted surface of the phenomena by estimating values at unobserved locations.</p>
<p>Statistical modeling of point reference data often assumes that the measurement variable is, theoretically, defined at locations that vary continuously across the domain. Thus, it necessitates specifying a random surface . A widely adopted approach involves modeling the surface as a realization of a stochastic process. Gaussian processes provide a practical framework for such modeling, offering a versatile tool for representing the spatial processes that vary continuously across space. The Gaussian processes facilitate straightforward inference and prediction by capturing spatial correlations, interpolating data, modeling variability, and enabling probabilistic inference. In the following, we will explore the application of Gaussian processes in analyzing point-referenced spatial data.</p>
</div>
<div id="modeling-point-referenced-spatial-data-using-gp" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Modeling point-referenced spatial data using GP<a href="introduction.html#modeling-point-referenced-spatial-data-using-gp" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Within this framework, observations over a finite set of locations in a spatial domain are assumed to be partial realizations of a spatial Gaussian process <span class="math inline">\(\{y(\mathbf{s}): \mathbf{s} \in \mathcal{D}\}\)</span> that defined on spatial index <span class="math inline">\(\mathbf{s}\)</span> varying continuously throughout <span class="math inline">\(d\)</span>–dimensional domain <span class="math inline">\(\mathcal{D} \in \mathbb{R}^{d}\)</span>. Therefore, the joint distribution of measurements at any finite set of locations is assumed to be multivariate normal, and the properties of the multivariate normal distribution ensure closed-form marginal and conditional distributions, leading to straightforward computation for model fitting and prediction.</p>
<p>It assumes that the measurement <span class="math inline">\(y(\mathbf{s})\)</span>, at location <span class="math inline">\(\mathbf{s}\)</span>, be a generated as</p>
<p><span class="math display">\[
\begin{align}
y(\mathbf{s}) = \mathbf{x}(\mathbf{s})^\prime \boldsymbol{\theta} + z(\mathbf{s}) + \epsilon(\mathbf{s}),
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\theta}\)</span> is a <span class="math inline">\(p\)</span>–dimensional vector of coefficient associated with <span class="math inline">\(p\)</span>–dimensional vector of covariates, <span class="math inline">\(\mathbf{x}(\mathbf{s})\)</span>, including intercept. The component <span class="math inline">\(z(\mathbf{s})\)</span> is assumed to be a zero mean isotropic Gaussian process with marginal variance <span class="math inline">\(\sigma^2\)</span> and correlation function <span class="math inline">\(\rho(\cdot)\)</span> capturing the spatial correlation and ensures that <span class="math inline">\(y(\mathbf{s})\)</span> is defined at every location <span class="math inline">\(\mathbf{s} \in \mathcal{D}\)</span>, and <span class="math inline">\(\epsilon(\mathbf{s})\)</span> is assumed to be an independent random measurement error which follows a normal distribution with mean zero and variance <span class="math inline">\(\tau^2\)</span>.</p>
<div id="inference-procedure" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Inference Procedure<a href="introduction.html#inference-procedure" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(\mathbf{y} = (y(\mathbf{s}_1),\ldots, y(\mathbf{s}_n))^\prime\)</span> be the <span class="math inline">\(n\)</span>–dimensional vector of data over <span class="math inline">\(n\)</span> locations <span class="math inline">\(\mathbf{s}_1,\ldots,\mathbf{s}_n\)</span> in <span class="math inline">\(d\)</span>–dimensional domain <span class="math inline">\(\mathcal{D} \in \mathbb{R}^{d}\)</span>. Using marginalization with respect to <span class="math inline">\(\mathbf{z}^\prime = (z(\mathbf{s}_1),\ldots,z(s_n))\)</span>, it can be shown that <span class="math inline">\(\mathbf{y}\)</span> is distributed as multivariate normal with</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}[\mathbf{y}] = \mathbf{X}\boldsymbol{\theta} \qquad \text{and} \qquad \text{Var}[\mathbf{y}] = \sigma^2 \mathbf{B} + \tau^2\mathbf{I},
\end{align}
\]</span>
where <span class="math inline">\(\mathbf{X}\)</span> is a <span class="math inline">\(n \times p\)</span> design matrix based on the vector of covariates <span class="math inline">\(\mathbf{x}(\mathbf{s}_i)\)</span> and <span class="math inline">\(\mathbf{B}\)</span> is the <span class="math inline">\(n\)</span>–dimensional correlation matrix whose <span class="math inline">\((i,j)\)</span>th elements is <span class="math inline">\(\mathbf{B}_{ij} = \rho(||\mathbf{s}_i-\mathbf{s}_j||)\)</span>.</p>
<p>Under the Bayesian paradigm, model specification is complete after assigning a prior distribution for <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(\ell\)</span> and <span class="math inline">\(\tau\)</span>. Then, following Bayes’ theorem, the joint posterior distribution of <span class="math inline">\(\boldsymbol{\Phi} = \{\boldsymbol{\theta}, \sigma, \ell, \tau\}\)</span> is proportional to</p>
<p><span class="math display">\[
\begin{align}
\pi(\boldsymbol{\Phi} \mid \mathbf{y}) \propto \mathcal{N}\left(\mathbf{y} \mid \mathbf{X}\boldsymbol{\theta}, \mathbf{V}\right) \; \pi(\boldsymbol{\Phi}),
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{V} = \sigma^2 \mathbf{B} + \tau^2\mathbf{I}\)</span> and <span class="math inline">\(\pi(\boldsymbol{\Phi})\)</span> denotes the prior distribution assigned to <span class="math inline">\(\boldsymbol{\Phi}\)</span>. In general, the distribution <span class="math inline">\(\pi(\boldsymbol{\Phi} \mid \mathbf{y})\)</span> does not have a closed-form, and Markov chain Monte Carlo (MCMC) sampling methods are commonly employed to approximate this distribution. These methods are straightforward to implement using modern statistical computing platforms such as <code>BUGS</code>, <code>JAGS</code>, <code>NIMBLE</code>, and <code>Stan</code>. MCMC methods provide samples from the posterior distribution, which can be used to estimate various summary statistics. Once samples from the posterior distribution are available, predictions to unobserved locations follow straightforwardly.</p>
<p>The described inference procedure utilizes a model that is marginalized with respect to the latent GP by integrating out the latent variable <span class="math inline">\(\mathbf{z}\)</span>. This approach allows the model to directly predict the observed responses, which is why it is referred to as a response GP or marginal GP.</p>
</div>
</div>
<div id="response-gp-in-stan" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Response GP in Stan<a href="introduction.html#response-gp-in-stan" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<pre><code>data {
  int&lt;lower=0&gt; n;
  int&lt;lower=0&gt; p;
  vector[n] y;
  matrix[n,p] X;
  array[n] vector[2] coords;
  
  vector&lt;lower=0&gt;[p] scale_theta;
  real&lt;lower=0&gt; scale_sigma;
  real&lt;lower=0&gt; scale_tau;
  
  real&lt;lower = 0&gt; a; // Shape parameters in the prior for ell
  real&lt;lower = 0&gt; b; // Scale parameters in the prior for ell
  
}

transformed data{
  
}

parameters {
  vector[p] theta_std;
  real&lt;lower=0&gt; ell;
  real&lt;lower=0&gt; sigma_std;
  real&lt;lower=0&gt; tau_std;
}

transformed parameters{
  vector[p] theta = scale_theta .* theta_std;
  real sigma = scale_sigma * sigma_std;
  real tau = scale_sigma * tau_std;
}

model {
  theta_std ~ std_normal();
  ell ~ inv_gamma(a,b);
  sigma_std ~ std_normal();
  tau_std ~ std_normal();
  vector[n] mu = X*theta;
  matrix[n,n] Sigma = gp_matern32_cov(coords, sigma, ell);
  matrix[n,n] L = cholesky_decompose(add_diag(Sigma, square(tau)));
  y ~ multi_normal_cholesky(mu, L);
}</code></pre>
<div id="spatial-interpolation" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Spatial Interpolation<a href="introduction.html#spatial-interpolation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One main interest in point-referenced spatial data analysis is obtaining a predicted surface for the process through pointwise prediction. Let <span class="math inline">\(\{\mathbf{s}_1^\star, \ldots, \mathbf{s}_{n^\star}^\star\}\)</span> be a set of <span class="math inline">\(n^\star\)</span> high resoluted grid locations covering <span class="math inline">\(\mathcal{D}\)</span> and suppose that vector of <span class="math inline">\(p\)</span> covariates values <span class="math inline">\(\mathbf{x}(\mathbf{s}^\star)\)</span> at each site is available. To obtain predicted surface along with reporting uncertainty measures, we need posterior predicted distribution of <span class="math inline">\(\mathbf{y}^\star = (y(\mathbf{s}_1^\star),\ldots, y(\mathbf{s}_{n^\star}^\star))^\prime\)</span>, conditional on the observed data <span class="math inline">\(\mathbf{y}\)</span>. For this purpose, consider the joint vector <span class="math inline">\((\mathbf{y}^{\star\prime},\mathbf{y}^\prime)\)</span>, under a Gaussian process assumption whose distribution is <span class="math inline">\((n^\star + n)\)</span>–dimensional multivariate normal. Consequently, the conditional distribution <span class="math inline">\(\mathbf{y}^\star\)</span> given <span class="math inline">\(\mathbf{y}\)</span> is <span class="math inline">\(n^\star\)</span>–dimensional multivariate normal with conditional mean and variance, respectively, given by</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}[\mathbf{y}^\star \mid \mathbf{y}] =
\mathbf{X}^\star\boldsymbol{\theta} + \mathbf{V}^{\text{pred-to-obs}} \mathbf{V}^{-1} (\mathbf{y} - \mathbf{X}\boldsymbol{\theta})\\
\text{and} \text{Var}[\mathbf{y}^\star \mid \mathbf{y}] = \mathbf{V}^\star - \mathbf{V}^{\text{pred-to-obs}} \mathbf{V}^{-1} \mathbf{V}^{\text{obs-to-pred}},
\end{align}
\]</span></p>
<p>which is used to perform the prediction, where <span class="math inline">\(\mathbf{X}^\star\)</span> is the <span class="math inline">\(n^\star \times p\)</span> design matrix of covariates at prediction locations. The covariance matrix <span class="math inline">\(\mathbf{V}^\star\)</span> is equal to <span class="math inline">\(\sigma^2 \mathbf{B}^\star + \tau^2 \mathbf{I}\)</span>, where <span class="math inline">\(\mathbf{B}^\star\)</span> denotes the <span class="math inline">\(n^\star\)</span>–dimensional spatial correlation matrix among the prediction locations. The component <span class="math inline">\(\mathbf{V}^{\text{pred-to-obs}}\)</span> is equal to <span class="math inline">\(\sigma^2 \mathbf{B}^{\text{pred-to-obs}}\)</span>, where <span class="math inline">\(\mathbf{B}^\text{pred-to-obs}\)</span> denotes the <span class="math inline">\(n^\star \times n\)</span> spatial correlation matrix between prediction and observed locations.</p>
<p>However, the above joint prediction is computationally expensive as the conditional distribution of a multivariate normal distribution of dimension <span class="math inline">\((n^\star+n)\)</span>, computing conditional mean <span class="math inline">\(\mu_{y(\mathbf{s}^\star) \mid \mathbf{y}}\)</span> and variance <span class="math inline">\(\sigma^2_{y(\mathbf{s}^\star) \mid \mathbf{y}}\)</span> involves expensive matrix calculations. In practice, this can be avoided by performing predictions for each unobserved location separately. In that case, at a generic prediction location <span class="math inline">\(\mathbf{s}^\star \in \mathcal{D}\)</span>, the posterior predictive distribution of <span class="math inline">\(y(\mathbf{s})\)</span> at <span class="math inline">\(\mathbf{s}^\star\)</span> is given by</p>
<p><span class="math display">\[
\begin{align}
\pi(y(\mathbf{s}^\star) \mid \mathbf{y}) = \int_{\boldsymbol{\Phi}} \mathcal{N}(y(\mathbf{s}^\star) \mid \mathbf{X}\boldsymbol{\theta}, \mathbf{V}) \pi(\boldsymbol{\Phi} \mid \mathbf{y}) \mathrm{d}\boldsymbol{\Phi}
\end{align}
\]</span></p>
<p>This procedure is known as a univariate prediction, each step of which involves calculating the matrix inversion of order <span class="math inline">\(n\)</span> and is still expensive if <span class="math inline">\(n\)</span> is large.</p>
</div>
<div id="recovery-of-the-latent-component" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Recovery of the Latent Component<a href="introduction.html#recovery-of-the-latent-component" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One might be interested in the posterior distribution of the latent spatial component <span class="math inline">\(z(\mathbf{s})\)</span>. The inference using joint posterior distribution in equation ignores the estimation of the latent vector <span class="math inline">\(\mathbf{z}^\prime = (z(\mathbf{s}_1), \ldots, z(\mathbf{s}_n))\)</span> during model fitting. Nevertheless, we can recover the distribution of vector <span class="math inline">\(\mathbf{z}\)</span> components via composition sampling once samples from the posterior distribution of the parameters are available. Note that the joint posterior distribution of <span class="math inline">\(\mathbf{z}\)</span> is</p>
<p><span class="math display">\[
\begin{align}
\pi(\mathbf{z} \mid \mathbf{y}) &amp;= \int \pi(\boldsymbol{\Phi}, \mathbf{z} \mid \mathbf{y}) \; \mathrm{d} \boldsymbol{\Phi}\\
&amp;= \int \pi(\mathbf{z} \mid \boldsymbol{\Phi}, \mathbf{y}) \; \pi(\boldsymbol{\Phi} \mid \mathbf{y}) \; \mathrm{d} \boldsymbol{\Phi},
\end{align}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{align}
\pi(\mathbf{z} \mid \boldsymbol{\Phi}, \mathbf{y})
&amp;\propto \mathcal{N}(\mathbf{z} \mid \mathbf{0}, \sigma^2 \mathbf{B}) \; \mathcal{N}\left(\mathbf{y} \mid \mathbf{X}\boldsymbol{\theta} + \mathbf{z}, \tau^2\mathbf{I}\right)\\
&amp;\propto \exp\left\{-\frac{1}{2\sigma^2} \mathbf{z}^\prime \mathbf{B}^{-1} \mathbf{z}\right\} \; \exp\left\{-\frac{1}{2\tau^2} (\mathbf{y} - \mathbf{X}\boldsymbol{\theta} -\mathbf{z})^\prime (\mathbf{y} - \mathbf{X}\boldsymbol{\theta} - \mathbf{z})\right\} \nonumber\\
&amp;\propto \exp\left\{-\frac{1}{2}\mathbf{z}^\prime \left(\frac{1}{\tau^2} \mathbf{I} + \frac{1}{\sigma^2}\mathbf{B}^{-1} \right) \mathbf{z} - \mathbf{z}^\prime \left(\frac{1}{\tau^2} \mathbf{I}\right) (\mathbf{y} - \mathbf{X}\boldsymbol{\theta})\right\},
\end{align}
\]</span>
which is the kernel of the multivariate normal distribution with mean and covariance,
<span class="math display">\[
\begin{align}
\mathbb{E}[\mathbf{z} \mid \mathbf{y}] &amp;= \left(\dfrac{1}{\tau^2} \mathbf{I} + \dfrac{1}{\sigma^2}\mathbf{B}^{-1} \right)^{-1} \left(\dfrac{1}{\tau^2} \mathbf{I}\right) (\mathbf{y} - \mathbf{X}\boldsymbol{\theta}),\\
\text{and}\; \text{Var}[\mathbf{z} \mid \mathbf{y}] &amp;= \left(\dfrac{1}{\tau^2} \mathbf{I} + \dfrac{1}{\sigma^2}\mathbf{B}^{-1} \right)^{-1},
\end{align}
\]</span></p>
<p>respectively. Therefore, posterior samples for <span class="math inline">\(\mathbf{z}\)</span> can be obtained by drawing samples from <span class="math inline">\(\pi(\mathbf{z} \mid \boldsymbol{\Phi}, \mathbf{y})\)</span> one-for-one for each posterior sample of <span class="math inline">\(\boldsymbol{\Phi}\)</span>. These are post-MCMC calculations; hence, sampling is not very expensive. Given the posterior samples for <span class="math inline">\(\mathbf{z}\)</span> associated with observed locations and <span class="math inline">\(\boldsymbol{\Phi}\)</span>, it is also possible to obtain samples of the distribution of <span class="math inline">\(n^\star\)</span>–dimensional vector <span class="math inline">\(\mathbf{z}^\star\)</span> of the values of <span class="math inline">\(z(\mathbf{s})\)</span> at unobserved locations <span class="math inline">\(\mathbf{s}_{1}^\star, \ldots, \mathbf{s}_{n^\star}^\star\)</span> via composition sampling. The procedure involves assuming joint vectors <span class="math inline">\((\mathbf{z}^{\star\prime}, \mathbf{z}^\prime)\)</span> which follows <span class="math inline">\((n^\star + n)\)</span>–dimensional multivariate normal distribution and conditional distribution of <span class="math inline">\(\mathbf{z}^\star\)</span> given <span class="math inline">\(\mathbf{z}\)</span> is used to draw samples for <span class="math inline">\(\mathbf{z}^\star\)</span>. The conditional distribution is <span class="math inline">\(n^\star\)</span>–dimensional multivariate normal with mean
<span class="math display">\[
\begin{align}
\mathbf{E}[\mathbf{z}^\star \mid \mathbf{z}] = \mathbf{B}^{\text{pred-to-obs}} \mathbf{B}^{-1} \mathbf{z}
\end{align}
\]</span>
and variance
<span class="math display">\[
\begin{align}
\text{Var}[\mathbf{z}^\star \mid \mathbf{z}] = \sigma^2 (\mathbf{B}^\star - \mathbf{B}^{\text{pred-to-obs}} \mathbf{B}^{-1} \mathbf{B}^{\text{obs-to-pred}}).
\end{align}
\]</span></p>
</div>
</div>
<div id="hierarchical-representation-of-the-above-model" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Hierarchical representation of the above model<a href="introduction.html#hierarchical-representation-of-the-above-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Note that the model above specification is referred to as the marginal or response Gaussian model, and the inference and prediction procedures are outlined based on it. However, this model can be represented hierarchically as follows:</p>
<p><span class="math display">\[
\begin{align}
\mathbf{y} \mid \boldsymbol{\theta}, \mathbf{z} \sim \mathcal{N} \left(\mathbf{X}\boldsymbol{\theta} + \mathbf{z}, \tau^2\mathbf{I}\right),
\end{align}
\]</span></p>
<p><span class="math display">\[\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \sigma^2\mathbf{B}).\]</span></p>
<p>In practice, the response Gaussian process model is often preferred for efficient parameter estimation, as it circumvents the need to estimate the latent vector <span class="math inline">\(\mathbf{z}\)</span> directly. Instead, in a Bayesian analysis, once posterior samples for the parameters are obtained, estimates for <span class="math inline">\(\mathbf{z}\)</span> can be recovered through composition sampling techniques.</p>
</div>
<div id="latent-gp-in-stan" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Latent GP in Stan<a href="introduction.html#latent-gp-in-stan" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<pre><code>data {
  int&lt;lower=0&gt; n;
  int&lt;lower=0&gt; p;
  vector[n] y;
  matrix[n,p] X;
  array[n] vector[2] coords;
  
  vector&lt;lower=0&gt;[p] scale_theta;
  real&lt;lower=0&gt; scale_sigma;
  real&lt;lower=0&gt; scale_tau;
  
  real&lt;lower=0&gt; a;
  real&lt;lower=0&gt; b;
}

transformed data{
  
}

parameters {
  vector[p] theta_std;
  real&lt;lower=0&gt; ell;
  real&lt;lower=0&gt; sigma_std;
  real&lt;lower=0&gt; tau_std;
  vector[n] noise;
}

transformed parameters{
  vector[p] theta = scale_theta .* theta_std;
  real sigma = scale_sigma * sigma_std;
  real tau = scale_sigma * tau_std;
  //vector[n] z = cholesky_decompose(add_diag(gp_matern32_cov(coords, sigma, ell), 1e-8)) * noise;
  vector[n] z = cholesky_decompose(add_diag(gp_exponential_cov(coords, sigma, ell), 1e-8)) * noise;
  
  //matrix[n,n] Sigma = gp_exponential_cov(coords, sigma, phi);
  //matrix[n,n] V = add_diag(V, 1e-8);
  //matrix[n,n] L = cholesky_decompose(V);
  //vector[n] z = L * noise;
}

model {
  theta_std ~ std_normal();
  ell ~ inv_gamma(a,b);
  sigma_std ~ std_normal();
  tau_std ~ std_normal();
  noise ~ std_normal();
  vector[n] mu = X*theta;
  
  y ~ normal(mu + z, tau);
}

generated quantities{
  
}</code></pre>
</div>
<div id="computational-complexity-in-analysing-large-datasets" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Computational complexity in analysing large datasets<a href="introduction.html#computational-complexity-in-analysing-large-datasets" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The evaluation of the likelihood of a GP-based model requires the inversion of an <span class="math inline">\(n \times n\)</span> covariance matrix, which has a computational complexity of <span class="math inline">\(\mathcal{O}(n^3)\)</span>, making it impractical for analyzing large datasets.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lowrank.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/paritoshkroy/ApproximateGP/edit/docs/01-Introduction.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-start.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
